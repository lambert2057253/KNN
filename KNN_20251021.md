# 🌸 K-Nearest Neighbors (KNN) 教學文件  
## (以 Python 物件導向與 scikit-learn 實作，IRIS 資料集為例)

---

## 📘 一、KNN 概念說明

**K-Nearest Neighbors (KNN)** 是一種基於「距離」的監督式學習演算法，常用於分類與回歸問題。  
其核心思想為：
> 「給定一筆未知類別的資料，找出與它最接近的 K 個已標記樣本，並根據這些樣本的類別投票決定結果。」

---

## 🧮 二、KNN 演算法流程

1. 選擇鄰居數量 `K`
2. 計算未知樣本與所有樣本的距離（常用歐氏距離）
3. 選擇距離最近的 `K` 筆資料
4. 根據多數投票決定分類（或平均數決定回歸）
5. 輸出預測結果

---

## 📊 三、IRIS 資料集簡介

Iris dataset 為 scikit-learn 內建的經典資料集，包含 150 筆資料、4 個特徵與 3 種花類別：

| 特徵名稱 | 說明 |
|-----------|------|
| sepal length (cm) | 花萼長度 |
| sepal width (cm)  | 花萼寬度 |
| petal length (cm) | 花瓣長度 |
| petal width (cm)  | 花瓣寬度 |

類別包含：
- **Setosa**
- **Versicolor**
- **Virginica**

---

## ⚙️ 四、Python 物件導向實作 (OOP Implementation)

```python
from sklearn.datasets import load_iris
import numpy as np
from collections import Counter

class KNNClassifier:
    """自訂 KNN 分類器 (物件導向版本)"""
    def __init__(self, k=3):
        self.k = k

    def fit(self, X_train, y_train):
        """儲存訓練資料"""
        self.X_train = X_train
        self.y_train = y_train

    def predict(self, X_test):
        """對測試資料進行預測"""
        predictions = [self._predict_one(x) for x in X_test]
        return np.array(predictions)

    def _predict_one(self, x):
        """計算單一樣本的最近鄰"""
        distances = np.linalg.norm(self.X_train - x, axis=1)
        k_indices = np.argsort(distances)[:self.k]
        k_neighbor_labels = self.y_train[k_indices]
        most_common = Counter(k_neighbor_labels).most_common(1)
        return most_common[0][0]

# 載入 IRIS 資料集
iris = load_iris()
X, y = iris.data, iris.target

# 切割訓練/測試集
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 建立模型並預測
model = KNNClassifier(k=5)
model.fit(X_train, y_train)
predictions = model.predict(X_test)

# 準確率
accuracy = np.mean(predictions == y_test)
print(f"KNN 自訂模型準確率: {accuracy:.2f}")
```

## 五、使用 scikit-learn 內建 KNN 模型
```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 建立模型
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# 預測
y_pred = knn.predict(X_test)

# 評估
acc = accuracy_score(y_test, y_pred)
print(f"scikit-learn KNN 準確率: {acc:.2f}")
```
## 八、可視化 (決策邊界範例)
```python
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

X_vis = X[:, :2]  # 只取前兩個特徵
knn_vis = KNeighborsClassifier(n_neighbors=5)
knn_vis.fit(X_vis, y)

# 建立網格
x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1
y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))

Z = knn_vis.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

plt.figure(figsize=(8,6))
plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=30)
plt.title("KNN 決策邊界 (K=5)")
plt.xlabel("Sepal length (cm)")
plt.ylabel("Sepal width (cm)")
plt.show()

```
